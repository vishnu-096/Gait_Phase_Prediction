{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-31 02:44:32.629120: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-31 02:44:32.629171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-31 02:44:32.630366: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-31 02:44:32.636757: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-31 02:44:33.281971: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn import metrics\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os, warnings, random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.layers as L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_gait_percentage(df, name):\n",
    "    percent = df[name].values.tolist()\n",
    "    X = np.zeros((len(df[name]),1))\n",
    "    Y = np.zeros((len(df[name]),1))\n",
    "    for i in range (len(percent)):\n",
    "        phi = percent[i] * 2 * math.pi / 100\n",
    "        X[i] = math.cos(phi)\n",
    "        Y[i] = math.sin(phi)\n",
    "    df['X'] = X\n",
    "    df['Y'] = Y\n",
    "  \n",
    "def convert_data(d_x, d_y, look_back = 1, fore_cast = 1):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "\n",
    "    for i in range(look_back, len(d_x) - fore_cast):\n",
    "        dataX.append(d_x[i - look_back: i])\n",
    "        dataY.append(d_y[i + fore_cast,:])\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def get_train_data_from_df(all_data, test_ratio):\n",
    "    all_data.info()\n",
    "    cycle = 0\n",
    "    cycle_list = []\n",
    "\n",
    "    start = 0\n",
    "\n",
    "    for i in range(len(all_data) - 1):\n",
    "        if (all_data['perc'][i+1] == 0):\n",
    "            cycle += 1\n",
    "            cycle_list.append(all_data.iloc[start:i])\n",
    "            start = i+1\n",
    "\n",
    "    random.shuffle(cycle_list)\n",
    "    source_table = pd.concat(cycle_list, axis=0, ignore_index=True)\n",
    "    source_table = source_table.drop([\"lgrf\", \"rgrf\", \"lcop\",\"rcop\"], axis = 1)\n",
    "    source_table\n",
    "    x = source_table\n",
    "    x = x.drop(['perc'], axis=1)\n",
    "    # scaler = MinMaxScaler()\n",
    "    # x_scaled = scaler.fit_transform(x)\n",
    "    # x_scaled = pd.DataFrame(x_scaled)\n",
    "\n",
    "    encode_gait_percentage(source_table, 'perc')\n",
    "    y = source_table[[\"X\",\"Y\"]]\n",
    "    # data_x = x_scaled\n",
    "    data_x=x.values\n",
    "    x.info()\n",
    "    data_y = y.values.reshape(-1,2)\n",
    "\n",
    "    X_train, X_test,y_train, y_test = train_test_split(data_x, data_y ,\n",
    "                            test_size=0.25,\n",
    "                            shuffle=False)\n",
    "    look_back = 10\n",
    "    fore_cast = 1\n",
    "\n",
    "    train_x, train_y = convert_data(X_train, y_train, look_back, fore_cast)\n",
    "    validation_x, validation_y = convert_data(X_test, y_test, look_back, fore_cast)\n",
    "\n",
    "    return train_x, train_y, validation_x, validation_y\n",
    "\n",
    "\n",
    "import random\n",
    "def get_data_frames_from_files(path, file_names, subject_dict, subjects):\n",
    "    file_list=[]\n",
    "    for i in range (len(file_names)):\n",
    "\n",
    "        subject = file_names[i].split('_')[0]\n",
    "        if not subject in subjects:\n",
    "            continue\n",
    "        else:\n",
    "            print(file_names[i])\n",
    "        leg_len = subject_dict[subject][0]\n",
    "        weight = subject_dict[subject][1]\n",
    "        tmp=pd.read_excel(path+ file_names[i], sheet_name='Sheet1')\n",
    "        perc_column = tmp['perc']\n",
    "        tmp = tmp.drop(columns=['perc'])\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    # Normalize each column separately\n",
    "        normalized_data = scaler.fit_transform(tmp)\n",
    "        column_names = tmp.columns\n",
    "\n",
    "        normalized_df = pd.DataFrame(normalized_data, columns=column_names)\n",
    "        tmp['l_ph_hip']=tmp['l_ph_hip']/300\n",
    "        tmp['r_ph_hip']=tmp['r_ph_hip']/300\n",
    "        tmp['l_ph_fo']=tmp['l_ph_fo']/300\n",
    "        tmp['r_ph_fo']=tmp['r_ph_fo']/300\n",
    "        tmp['lcop']= tmp['lcop']*1000\n",
    "        tmp['rcop']=tmp['rcop']*1000\n",
    "        tmp['strike_frame']=tmp['strike_frame']/400\n",
    "        tmp['st_sw_phase']=tmp['st_sw_phase']/200\n",
    "        normalized_df =tmp\n",
    "        normalized_df['leg_len']=leg_len\n",
    "        normalized_df['weight']=weight\n",
    "        normalized_df['perc']= perc_column\n",
    "        # normalized_df.insert(tmp.columns.get_loc('col1'), 'perc', perc_column)\n",
    "\n",
    "        file_list.append(normalized_df)\n",
    "\n",
    "    random.shuffle(file_list)\n",
    "\n",
    "    all_data = pd.concat(file_list, axis=0, ignore_index=True)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "from keras import Model\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "import keras\n",
    "from keras.layers import Input, Dense, SimpleRNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import mean_squared_error\n",
    "\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1),\n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1),\n",
    "                               initializer='zeros', trainable=True)\n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        print(x.shape)\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)\n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        # context = K.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "class multiAttentionHead(Layer):\n",
    "    def __init__(self, num_heads=10, k_dim=64, use_bias=False, **kwargs):\n",
    "        self.k_dim = self.q_dim = self.v_dim = k_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_bias = use_bias\n",
    "        super(multiAttentionHead,self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.f_dim = input_shape[-1]\n",
    "        #[B,token,feature_dim]*[feature_dim,num_heads,v_dim/q_dim/k_dim]->[B,token,num_heads,q_dim/k_dim/v_dim]\n",
    "        if self.use_bias:\n",
    "            self.query_dense = einsum_dense.EinsumDense('abc,cde->abde', output_shape=[None, self.num_heads, self.q_dim], bias_axes='de')\n",
    "            self.key_dense = einsum_dense.EinsumDense('abc,cde->abde', output_shape=[None, self.num_heads, self.k_dim], bias_axes='de')\n",
    "            self.value_dense = einsum_dense.EinsumDense('abc,cde->abde', output_shape=[None, self.num_heads, self.v_dim], bias_axes='de')\n",
    "            #[B,token,num_heads,v_dim]*[num_heads,v_dim,feature_dim]->[B,token,feature_dim]\n",
    "            self.Wo = einsum_dense.EinsumDense('abcd,cde->abe', output_shape=[None, self.f_dim], bias_axes='e')\n",
    "        else:\n",
    "            self.query_dense = einsum_dense.EinsumDense('abc,cde->abde', output_shape=[None, self.num_heads, self.q_dim])\n",
    "            self.key_dense = einsum_dense.EinsumDense('abc,cde->abde', output_shape=[None, self.num_heads, self.k_dim])\n",
    "            self.value_dense = einsum_dense.EinsumDense('abc,cde->abde', output_shape=[None, self.num_heads, self.v_dim])\n",
    "            #[B,token,num_heads,v_dim]*[num_heads,v_dim,feature_dim]->[B,token,feature_dim]\n",
    "            self.Wo = einsum_dense.EinsumDense('abcd,cde->abe', output_shape=[None, self.f_dim])\n",
    "        super(multiAttentionHead, self).build(input_shape)\n",
    "    \n",
    "    def call(self, input_vec, attention_mask=None):\n",
    "        query = self.query_dense(input_vec)#[B,token,num_heads,q_dim]\n",
    "        key = self.key_dense(input_vec)#[B,token,num_heads,k_dim]\n",
    "        value = self.value_dense(input_vec)#[B,token,num_heads,v_dim]\n",
    "        #[B,token,num_heads,q_dim]*[B,token,num_heads,k_dim]->[B,num_heads,token,token]\n",
    "        scaleddotproduct =  special_math_ops.einsum('abcd,aecd->acbe', query, key)\n",
    "        scaleddotproduct = tf.math.divide(scaleddotproduct, float(math.sqrt(self.k_dim)))\n",
    "        if attention_mask:\n",
    "            scaleddotproduct = tf.where(attention_mask, scaleddotproduct, -1e9)\n",
    "        softmax = tf.nn.softmax(scaleddotproduct, axis=-1)\n",
    "        #[B,num_heads,token,token]*[B,token,num_heads,v_dim]->[B,token,num_heads,v_dim]\n",
    "        softmax_value = special_math_ops.einsum('acbe,aecd->abcd', softmax, value)\n",
    "        #[B,token,num_heads,v_dim]*[num_heads,v_dim,feature_dim]->[B,token,feature_dim]\n",
    "        final = self.Wo(softmax_value)\n",
    "        return final    \n",
    "\n",
    "class ElementwiseMultiply(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(ElementwiseMultiply, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\"kernel\", (self.units,), initializer=\"glorot_uniform\", trainable=True)\n",
    "        super(ElementwiseMultiply, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs * self.kernel\n",
    "\n",
    "    \n",
    "class Sampling(L.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, p = inputs\n",
    "        p1=tf.reduce_sum(p[0])\n",
    "        # p1=.\n",
    "        p2=tf.reduce_sum(p[1])\n",
    "        # print(p1)\n",
    "        # print(p2)\n",
    "        # weights_length_values = p.numpy()\n",
    "        # print(weights_length_values)\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        seq = tf.shape(z_mean)[1]\n",
    "        dim = tf.shape(z_mean)[2]\n",
    "        epsilon = tf.random.normal(shape=(batch, seq, dim))\n",
    "        return p1*z_mean + tf.exp(0.5 * z_log_var) * epsilon*p2\n",
    "   \n",
    "def motion_encoder_model():\n",
    "    seq_len =10\n",
    "    n_features=train_x.shape[2]\n",
    "    inp=Input(shape=(seq_len, n_features))\n",
    "\n",
    "def encoder_model():\n",
    "    seq_len=10\n",
    "    n_features=train_x.shape[2]\n",
    "    x=Input(shape=(seq_len, n_features))\n",
    "    part_1 = x[:, :, :n_features-2]\n",
    "\n",
    "    part_2 = x[0, 0, n_features-2:]\n",
    "\n",
    "    part_3 = x[:, :, n_features-7:n_features-2]\n",
    "    # l2=tf.keras.layers.AveragePooling1D(\n",
    "    #     pool_size=2,\n",
    "    #     strides=1, padding=\"same\")(part_3)\n",
    "    # lin_l2=L.Dense(8)(part_3)\n",
    "    lin_l2 = attention()(part_3)\n",
    "    lin_l3=L.Dense(4)(lin_l2)\n",
    "\n",
    "    LSTM_layer2 = LSTM(8, return_sequences=True)(lin_l2)\n",
    "\n",
    "    print(part_2)\n",
    "    l1=tf.keras.layers.Conv1D(16, 2, padding=\"same\")(part_1)\n",
    "    # weighted_param=L.Dense(2)(part_2)\n",
    "    weighted_param = ElementwiseMultiply(units=2)(part_2)\n",
    "\n",
    "    # l1=tf.keras.layers.AveragePooling1D(\n",
    "    #     pool_size=2,\n",
    "    #     strides=1, padding=\"same\")(part_1)\n",
    "    # lin_l1=L.Dense(4)(l1)\n",
    "    # att_multi =multiAttentionHead(num_heads=4,k_dim=2,use_bias=True)\n",
    "    \n",
    "    # att_op = att_multi(l1)\n",
    "    att_op=attention()(l1)\n",
    "    # att1 = attention()(part_1)\n",
    "    # layer_normed = L.LayerNormalization(axis=-1)(att_op)\n",
    "\n",
    "    # tmp_inp=L.Concatenate()([att1, part_2])\n",
    "\n",
    "    # rep_layer = L.RepeatVector((seq_len))(att1);\n",
    "    # tmp_inp=L.Concatenate()([att1, part_2])\n",
    "\n",
    "    # latent_sp=L.TimeDistributed(Dense(4))(rep_layer)\n",
    "    # f1=tf.keras.layers.Flatten()(l1)\n",
    "    # print(l1.shape)\n",
    "    # lstm_inp=L.Concatenate(axis=2)([lin_l1, rep_layer, part_2])\n",
    "    # RNN_layer = SimpleRNN(hidden_units, return_sequences=True, activation=activation)(x)\n",
    "    LSTM_layer1 = LSTM(32, return_sequences=True)(att_op)\n",
    "    # LSTM_layer2 = LSTM(8, return_sequences=True)(LSTM_layer1)\n",
    "\n",
    "    concat_layer=L.Concatenate(axis=2)([LSTM_layer1, lin_l3])\n",
    "    # layer_normed = L.LayerNormalization(axis=-1)(LSTM_layer2)\n",
    "\n",
    "    # attn_layer1 = attention()(LSTM_layer2)\n",
    "    mean = L.Dense(2)(concat_layer)\n",
    "    log_var= L.Dense(2)(concat_layer)\n",
    "    z = Sampling()([mean, log_var, weighted_param])\n",
    "    latent_sp=L.TimeDistributed(L.Dense(4))(concat_layer)\n",
    "\n",
    "    encoder = tf.keras.Model(x, (mean, log_var, z, latent_sp), name=\"Encoder\")\n",
    "    return encoder  \n",
    "    # seq_len=10\n",
    "    # n_features=train_x.shape[2]\n",
    "    # x=Input(shape=(seq_len, n_features))\n",
    "    # part_1 = x[:, :, :n_features-2]\n",
    "\n",
    "    # part_2 = x[0, 0, n_features-2:]\n",
    "\n",
    "    # part_3 = x[:, :, n_features-5:n_features-2]\n",
    "    # # l2=tf.keras.layers.AveragePooling1D(\n",
    "    # #     pool_size=2,\n",
    "    # #     strides=1, padding=\"same\")(part_3)\n",
    "    # lin_l2=L.Dense(8)(part_3)\n",
    "    # lin_l3=L.Dense(4)(lin_l2)\n",
    "\n",
    "    # # LSTM_layer2 = LSTM(16, return_sequences=True)(lin_l2)\n",
    "\n",
    "    # print(part_2)\n",
    "    # l1=tf.keras.layers.AveragePooling1D(\n",
    "    #     pool_size=2,\n",
    "    #     strides=1, padding=\"same\")(part_1)\n",
    "    # lin_l1=L.Dense(4)(l1)\n",
    "\n",
    "    # att1 = attention()(part_1)\n",
    "    # # tmp_inp=L.Concatenate()([att1, part_2])\n",
    "\n",
    "    # rep_layer = L.RepeatVector((seq_len))(att1);\n",
    "    # # tmp_inp=L.Concatenate()([att1, part_2])\n",
    "\n",
    "    # # latent_sp=L.TimeDistributed(Dense(4))(rep_layer)\n",
    "    # # f1=tf.keras.layers.Flatten()(l1)\n",
    "    # # print(l1.shape)\n",
    "    # # lstm_inp=L.Concatenate(axis=2)([lin_l1, rep_layer, part_2])\n",
    "    # # RNN_layer = SimpleRNN(hidden_units, return_sequences=True, activation=activation)(x)\n",
    "    # LSTM_layer1 = LSTM(32, return_sequences=True)(rep_layer)\n",
    "    # # LSTM_layer2 = LSTM(8, return_sequences=True)(LSTM_layer1)\n",
    "\n",
    "    # concat_layer=L.Concatenate(axis=2)([LSTM_layer1, lin_l3])\n",
    "\n",
    "    # # attn_layer1 = attention()(LSTM_layer2)\n",
    "    # mean = L.Dense(3)(concat_layer)\n",
    "    # log_var= L.Dense(3)(concat_layer)\n",
    "    # z = Sampling()([mean, log_var, part_2])\n",
    "    # latent_sp=L.TimeDistributed(L.Dense(4))(concat_layer)\n",
    "\n",
    "    # encoder = tf.keras.Model(x, (mean, log_var, z, latent_sp), name=\"Encoder\")\n",
    "    # return encoder      \n",
    "\n",
    "def decoder_model():\n",
    "        \n",
    "    latent_dim =(10,8)\n",
    "    n_features=train_x.shape[2]\n",
    "\n",
    "    n_real_features = n_features -2\n",
    "    input_1_shape=(10,2)\n",
    "    input_2_shape=( 10,4)\n",
    "    # input_3_shape=( 10,3)\n",
    "    input1 = tf.keras.Input(shape=input_1_shape, name='input_layer1')\n",
    "    input2 = tf.keras.Input(shape=input_2_shape, name='input_layer2')\n",
    "    concat1= L.Concatenate(axis=2)\n",
    "    concatenated_input = L.Concatenate(axis=2)([input1, input2])\n",
    "    # pooled_l = L.AveragePooling2D(pool_size=)\n",
    "    # f=Flatten()(concatenated_input)\n",
    "    # print(f)\n",
    "    dec_l1 = L.Dense(8)(concatenated_input)\n",
    "    # rep_layer = L.RepeatVector((seq_len))(dec_l1);\n",
    "    dec_LSTM_layer1 = LSTM(32, return_sequences=True)(dec_l1)\n",
    "    dec_LSTM_layer2 = LSTM(8, return_sequences=True)(dec_LSTM_layer1)\n",
    "\n",
    "    lin_layer = L.TimeDistributed(L.Dense(n_real_features))(dec_LSTM_layer2)\n",
    "\n",
    "    # tmp_layer = Flatten()(dec_LSTM_layer)\n",
    "    # lin_layer = L.Dense(2)(tmp_layer)\n",
    "    decoder = tf.keras.Model([input1, input2], lin_layer, name=\"Decoder\")\n",
    "    return decoder\n",
    "\n",
    "\n",
    "seq_len=10\n",
    "n_features=11\n",
    "train_x=np.zeros((10,10,11))\n",
    "#encoder_model().summary()\n",
    "#decoder_model().summary()\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z, ls = self.encoder(data)\n",
    "            # inp_dec = tf.concat((z, ls, p2), axis=2)\n",
    "            # print(inp_dec)\n",
    "            reconstruction = self.decoder([z,ls])\n",
    "            # tmp1=keras.losses.binary_crossentropy(data, reconstruction)\n",
    "            # reconstruction_loss = K.mean(K.square(data - reconstruction))\n",
    "            mse = tf.keras.losses.MeanSquaredError()\n",
    "            n_features = data.shape[2] \n",
    "            real_data = data[:,:,0:n_features-2]\n",
    "            reconstruction_loss = mse(real_data, reconstruction)\n",
    "            # reconstruction_loss = tf.reduce_mean(\n",
    "            #     tf.reduce_sum(\n",
    "            #         keras.losses.binary_crossentropy(data, reconstruction)\n",
    "            #     )\n",
    "            # )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=(1,2)))\n",
    "            total_loss = 1.5*reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "        \n",
    "def train_vae(train_x):\n",
    "    enc=encoder_model()\n",
    "    dec=decoder_model()\n",
    "    vae = VAE(enc, dec)\n",
    "    vae.compile(optimizer=keras.optimizers.Adam())\n",
    "    vae.fit(train_x, epochs=22, batch_size=128, verbose=2)\n",
    "\n",
    "    return vae.encoder,vae.decoder\n",
    "\n",
    "\n",
    "def test_model_get_results(encoder, mlp_model, validation_x, validation_y, display_flag, tag, file):\n",
    "    print(\"validation_x shape\",validation_x.shape)\n",
    "    _,_,samp_v, ls_v=encoder.predict(validation_x)\n",
    "    ls_v = tf.concat((samp_v, ls_v), axis=2)\n",
    "    print('Encoded time-series shape', ls_v.shape)\n",
    "    testPredict = mlp_model.predict(ls_v)\n",
    "    print(validation_y.shape)\n",
    "    print(testPredict.shape)\n",
    "    testScore = np.sqrt(mean_squared_error(validation_y, testPredict))\n",
    "    # print(testScore)\n",
    "    # print('Test Score: %.2f RMSE' % (testScore))\n",
    "    pred = np.zeros((len(testPredict),1))\n",
    "\n",
    "    for iter in range(len(testPredict)):\n",
    "        x = testPredict[iter][0]\n",
    "        y = testPredict[iter][1]\n",
    "        pred[iter] = ((math.atan2(y,x) + 2*math.pi) % (2*math.pi)) * (100 / (2*math.pi))\n",
    "\n",
    "    actual = np.zeros((len(validation_y),1))\n",
    "\n",
    "    for iter in range(len(validation_y)):\n",
    "        x =validation_y[iter][0]\n",
    "        y =validation_y[iter][1]\n",
    "        # st_frame = validation_x[0,9,7]\n",
    "\n",
    "        actual[iter] = ((math.atan2(y,x) + 2*math.pi) % (2*math.pi)) * (100 / (2*math.pi))\n",
    "        # if st_frame>100 and pred[iter]<15:\n",
    "        #     pred[iter]=100-pred[iter]\n",
    "        # if st_frame<90 and pred[iter]>85:\n",
    "        #     pred[iter]=100-pred[iter]\n",
    "\n",
    "\n",
    "    cor_actual=[]\n",
    "    cor_pred=[]\n",
    "    prec_list=[]\n",
    "\n",
    "    for i in range(5):\n",
    "        correct = 0\n",
    "        for iter in range(len(actual)):\n",
    "            # if (actual[iter]>98) or(actual[iter]<3):\n",
    "            #     correct+=1\n",
    "            #     continue\n",
    "            if (abs(actual[iter] - pred[iter]) <= (i+1)):\n",
    "                correct+=1\n",
    "            cor_pred.append(pred[iter])\n",
    "            cor_actual.append(actual[iter])\n",
    "        prec=correct * 100/len(actual)\n",
    "        print(\"Precision \", i+1, \": \", prec)\n",
    "        # file.write(str(prec))\n",
    "        # file.write(\"\\n\")\n",
    "        prec_list.append(prec)\n",
    "\n",
    "    rmse = 0\n",
    "    length = len(actual)\n",
    "    for i in range(len(actual)):\n",
    "        if abs (pred[i] - actual[i]) >=90:\n",
    "            length -= 1\n",
    "        else:\n",
    "            rmse = rmse + pow(pred[i] - actual[i], 2)\n",
    "    rmse = rmse / length\n",
    "    rmse = math.sqrt(rmse)\n",
    "\n",
    "    if os.path.exists(result_pkl_file):\n",
    "        with open(result_pkl_file,'rb') as pkl_file:\n",
    "            res_dict=pickle.load(pkl_file)\n",
    "    else:\n",
    "        res_dict={}\n",
    "    result={}\n",
    "    result['rmse']=rmse\n",
    "    result['prec_list']=prec_list\n",
    "    res_dict[tag]=result\n",
    "    with open(result_pkl_file, 'wb')as pkl_file:\n",
    "        pickle.dump(res_dict, pkl_file)\n",
    "\n",
    "\n",
    "    print(rmse)\n",
    "    # file.write(\"rmse \"+str(rmse))\n",
    "    # file.write(\"\\n\")\n",
    "    if display_flag:\n",
    "        plt.scatter(cor_actual, cor_pred, facecolors='none', edgecolors='crimson',alpha=0.4)\n",
    "        p1 = max(max(cor_pred), max(cor_actual))\n",
    "        p2 = min(min(cor_pred), min(cor_actual))\n",
    "\n",
    "        ci = 0.1 * np.std([p1,p2]) / np.mean([p1,p2])\n",
    "\n",
    "        plt.plot([p1, p2], [p1, p2], 'b-', linewidth =3)\n",
    "        plt.title('Actual vs Prediction')\n",
    "        plt.savefig(result_path+tag+\"__res.png\")\n",
    "\n",
    "    return prec_list, rmse\n",
    "\n",
    "\n",
    "def train_mlp_model(samp_t, train_y):\n",
    "    mlp_model = Sequential()\n",
    "\n",
    "    mlp_model.add(tf.keras.Input(shape=(samp_t.shape[1], samp_t.shape[2]), name='input_layer'))\n",
    "    mlp_model.add(LSTM(32))\n",
    "\n",
    "    mlp_model.add(L.Dense(32, kernel_initializer='glorot_normal', activation='relu'))\n",
    "    # mlp_model.add(L.Dense(32, kernel_initializer='glorot_normal', activation='relu', input_dim=(samp_t.shape[1]*samp_t.shape[2])))\n",
    "    # mlp_model.add(L.Dense(32, kernel_initializer='glorot_normal', activation='relu', input_dim=(train_encoded.shape[1])))\n",
    "\n",
    "    mlp_model.add(L.Dense(8, kernel_initializer='glorot_normal', activation='relu'))\n",
    "    mlp_model.add(L.Dense(2))\n",
    "    mlp_model.summary()\n",
    "\n",
    "    initial_learning_rate = 0.001\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=100000,\n",
    "        decay_rate=0.96,\n",
    "        staircase=True)\n",
    "    adam = optimizers.Adam(lr_schedule)\n",
    "\n",
    "    mlp_model.compile(loss='mse', optimizer=adam)\n",
    "\n",
    "    epochs = 25\n",
    "    batch=64                                                                                                                                                           \n",
    "    # lrate = LearningRateScheduler(step_decay)\n",
    "    monitor = EarlyStopping(monitor='loss', min_delta=1e-5, patience=3)\n",
    "    # train_encoded_reshaped=np.reshape(train_encoded,(train_encoded.shape[0], train_encoded.shape[1]*train_encoded.shape[2]))\n",
    "\n",
    "    # train_encoded_reshaped=np.reshape(train_encoded,(train_encoded.shape[0], train_encoded.shape[1]))\n",
    "    # train_encoded_reshaped = train_encoded\n",
    "    callback_list = [monitor]\n",
    "    mlp_history = mlp_model.fit(samp_t , train_y, callbacks=callback_list, epochs=epochs, batch_size=batch,  verbose=2)\n",
    "\n",
    "    return mlp_model\n",
    "\n",
    "def create_latent_space_train_mlp(encoder, train_x):\n",
    "  print(\"OKAY till here 2\")\n",
    "  _,_,samp_t, ls_t=encoder.predict(train_x)\n",
    "  print(\"OKAY till here 3\")\n",
    "\n",
    "  ls_t = tf.concat((samp_t, ls_t), axis=2)\n",
    "  print('Encoded time-series shape', ls_t.shape)\n",
    "  return ls_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file : JL_I_0_new_.xlsx\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'st_sw_phase_l'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/clean_gait_phase/lib/python3.10/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'st_sw_phase_l'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m tmp\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_excel(path\u001b[38;5;241m+\u001b[39m file_name, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSheet1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m perc_column \u001b[38;5;241m=\u001b[39m tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 60\u001b[0m st_sw_col_l \u001b[38;5;241m=\u001b[39m \u001b[43mtmp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mst_sw_phase_l\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     61\u001b[0m st_sw_col_r\u001b[38;5;241m=\u001b[39m tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mst_sw_phase_r\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     62\u001b[0m lhip_col \u001b[38;5;241m=\u001b[39m tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlhip_ang\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/clean_gait_phase/lib/python3.10/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/clean_gait_phase/lib/python3.10/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'st_sw_phase_l'"
     ]
    }
   ],
   "source": [
    "file_names = ['JL_I_0_new_.xlsx', 'JL_I_2_new_.xlsx','JL_I_3_new_.xlsx','JL_I_5_new_.xlsx','JL_I_4_new_.xlsx',\n",
    "              'JS_I_0_new_.xlsx', 'JS_I_2_new_.xlsx','JS_I_3_new_.xlsx','JS_I_5_new_.xlsx','JS_I_4_new_.xlsx',\n",
    "              'AK_I_0_new_.xlsx', 'AK_I_2_new_.xlsx','AK_I_3_new_.xlsx','AK_I_5_new_.xlsx','AK_I_4_new_.xlsx',\n",
    "              'VN_I_0_new_.xlsx', 'VN_I_2_new_.xlsx','VN_I_3_new_.xlsx','VN_I_5_new_.xlsx','VN_I_4_new_.xlsx',\n",
    "              'VP_I_0_new_.xlsx', 'VP_I_2_new_.xlsx','VP_I_3_new_.xlsx','VP_I_5_new_.xlsx','VP_I_4_new_.xlsx',\n",
    "             'SOE_I_0_new_.xlsx', 'SOE_I_2_new_.xlsx','SOE_I_3_new_.xlsx','SOE_I_5_new_.xlsx', 'SOE_I_4_new_.xlsx', 'SD_I_3_new_.xlsx', 'SD_I_4_new_.xlsx','SD_I_5_new_.xlsx',\n",
    "             'SD_I_0_new_.xlsx','SD_I_2_new_.xlsx','TH_I_0_new_.xlsx', 'TH_I_2_new_.xlsx', 'TH_I_3_new_.xlsx','TH_I_4_new_.xlsx', 'TH_I_5_new_.xlsx'\n",
    "             ,'PK_I_0_new_.xlsx', 'PK_I_2_new_.xlsx', 'PK_I_3_new_.xlsx','PK_I_5_new_.xlsx',\n",
    "              'SKS_0_I_new_.xlsx', 'SKS_2_I_new_.xlsx','SKS_3_I_new_.xlsx','SKS_4_I_new_.xlsx','SKS_5_I_new_.xlsx',\n",
    "            'PH_I_0_new_.xlsx',  'PH_I_2_new_.xlsx',  'PH_I_3_new_.xlsx',  'PH_I_4_new_.xlsx',  'PH_I_5_new_.xlsx',\n",
    "            'YC_I_0_new_.xlsx',  'YC_I_2_new_.xlsx',  'YC_I_3_new_.xlsx',  'YC_I_4_new_.xlsx',  'YC_I_5_new_.xlsx'\n",
    "              ]\n",
    "subject_dict = {'VN':[0.90,0.63],'AK':[0.80,0.57],'JS':[0.89,0.64],'JL':[0.79,0.63],'SKS':[0.83, 0.58],'VP':[0.93, 0.77],'SOE':[0.90, 0.83],\n",
    "                'SD':[0.83, 0.70], 'TH':[0.66, 0.52], 'PK':[0.90, 0.88], 'PH':[0.95,0.80], 'YC':[0.82,0.79]}\n",
    "subject_names = ['VP','AK', 'SD','PH', 'YC','JL','JS', 'PK', 'SOE', 'TH','SKS', 'VN']#,'VN','AK' 'SOE'\n",
    "sub_comb_list=[]\n",
    "test_sub_list=[]\n",
    "acc_list=[]\n",
    "rmse_list=[]\n",
    "test_acc_list=[]\n",
    "test_rmse_list=[]\n",
    "\n",
    "\n",
    "#path=\"/home/vtp/Gait_Phase_Prediction/Subject_data/Final_files/\"\n",
    "#result_path = \"/home/vtp/Gait_Phase_Prediction/Results/final/\"\n",
    "\n",
    "path = \"/home/hikikomori/Gait_Phase_Prediction/Data/Incline/all subject/\"\n",
    "result_path = \"/home/hikikomori/Gait_Phase_Prediction/Results/final/\"\n",
    "\n",
    "result_pkl_file=result_path+'results_ver_hip.pkl'\n",
    "pkl_file=path+\"all_sub_vae_ver_hip_cop.pkl\"\n",
    "# pkl_file=path+\"good_sub_data.pkl\"\n",
    "\n",
    "for sub in subject_names:\n",
    "\n",
    "  test_sub_list.append(sub)\n",
    "  tmp=subject_names.copy()\n",
    "  tmp.remove(sub)\n",
    "  sub_comb_list.append(tmp)\n",
    "\n",
    "df_dict={}\n",
    "\n",
    "if os.path.exists(pkl_file):\n",
    "    # File is already in pickle format, read to dict\n",
    "    with open(pkl_file, 'rb') as file:\n",
    "        df_dict = pickle.load(file)\n",
    "    \n",
    "else:\n",
    "    # File is not in pickle format/ does not exist, convert and save it as a pickle file\n",
    "\n",
    "    for file_name in file_names:\n",
    "        subject = file_name.split('_')[0]\n",
    "        if subject not in subject_names:\n",
    "            continue\n",
    "        leg_len = subject_dict[subject][0]\n",
    "        weight = subject_dict[subject][1]\n",
    "        print(\"Reading file :\", file_name)\n",
    "        tmp=pd.read_excel(path+ file_name, sheet_name='Sheet1')\n",
    "        perc_column = tmp['perc']\n",
    "        st_sw_col_l = tmp['st_sw_phase_l']\n",
    "        st_sw_col_r= tmp['st_sw_phase_r']\n",
    "        lhip_col = tmp['lhip_ang']\n",
    "        rhip_col = tmp['rhip_ang']\n",
    "        lhip_df=tmp['l_ph_hip']\n",
    "        rhip_df=tmp['r_ph_hip']\n",
    "        \n",
    "        st_l_l_col = tmp[['st_l_l']]\n",
    "        st_l_r_col = tmp[['st_l_r']]\n",
    "\n",
    "        tmp = tmp.drop(columns=['perc', 'st_sw_phase_l', 'st_sw_phase_r', 'lhip_ang', 'rhip_ang','l_ph_hip','r_ph_hip', 'st_l_l', 'st_l_r'])\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        tmp['l_ph_hip'] = lhip_df\n",
    "        tmp['r_ph_hip'] = rhip_df\n",
    "        tmp['st_sw_phase_l'] = st_sw_col_l\n",
    "        tmp['st_sw_phase_r'] = st_sw_col_l\n",
    "        tmp['st_l_l'] = scaler.fit_transform( st_l_l_col)\n",
    "        tmp['st_l_r'] = scaler.fit_transform( st_l_r_col)\n",
    "\n",
    "        column_names = tmp.columns\n",
    "\n",
    "\n",
    "        tmp['l_ph_hip']=tmp['l_ph_hip']/300\n",
    "        tmp['r_ph_hip']=tmp['r_ph_hip']/300\n",
    "        # tmp['l_ph_fo']=tmp['l_ph_fo']/300\n",
    "        # tmp['r_ph_fo']=tmp['r_ph_fo']/300\n",
    "        tmp['lcop']= tmp['lcop']*1000\n",
    "        tmp['rcop']=tmp['rcop']*1000\n",
    "        tmp['st_sw_phase_l']=tmp['st_sw_phase_l']/200\n",
    "        tmp['st_sw_phase_r']=tmp['st_sw_phase_r']/200\n",
    "\n",
    "        normalized_df =tmp\n",
    "\n",
    "        normalized_df['leg_len']=leg_len\n",
    "        normalized_df['weight']=weight\n",
    "        normalized_df['perc']= perc_column\n",
    "\n",
    "        df_dict[subject] = normalized_df\n",
    "    with open(pkl_file,'wb') as pickle_file:\n",
    "        pickle.dump(df_dict, pickle_file)\n",
    "\n",
    "sub_cnt_file='sub_cnt.pkl'\n",
    "with open(result_path+\"all_results_vae2.txt\",\"w\") as file:  \n",
    "    for a_ in range(len(subject_names)):\n",
    "        sub_file=open(sub_cnt_file,'rb')\n",
    "        cnt=pickle.load(sub_file)\n",
    "        sub_file.close()\n",
    "        if cnt==len(subject_names):\n",
    "            print(\"Done with all subjects !!\")\n",
    "            break\n",
    "        sub_comb=sub_comb_list[cnt]\n",
    "        sub_iter=cnt\n",
    "        print(\"Subject combination :\",sub_comb)\n",
    "        sub_tag=test_sub_list[sub_iter]\n",
    "        # all_data=get_data_frames_from_files(path, file_names, subject_dict, sub_comb)\n",
    "        file_list=[]\n",
    "        for sub in sub_comb:\n",
    "          tmp_df=df_dict[sub]\n",
    "          file_list.append(tmp_df)\n",
    "        random.shuffle(file_list)\n",
    "        all_data = pd.concat(file_list, axis=0, ignore_index=True)\n",
    "        train_x, train_y, validation_x, validation_y = get_train_data_from_df(all_data, 0.25)\n",
    "        print(\"Nan in train x at indices:\",np.argwhere(np.isnan(train_x)))\n",
    "        print(\"Nan in train y at indices:\",np.argwhere(np.isnan(train_y)))\n",
    "\n",
    "        print(train_x.shape)\n",
    "        print(validation_x.shape)\n",
    "        print(\"SSS\")\n",
    "        seq_len = train_x.shape[1]\n",
    "        n_features = train_x.shape[2]\n",
    "        encoder=[]\n",
    "        decoder=[]\n",
    "        mlp_model=[]\n",
    "        encoder,decoder = train_vae(train_x)\n",
    "        print(\"OKAY till here 1\")\n",
    "\n",
    "        samp_t=create_latent_space_train_mlp(encoder, train_x)\n",
    "        mlp_model = train_mlp_model(samp_t, train_y)\n",
    "        file.write(\"Training Result :\")\n",
    "        file.write(\"\\n\")\n",
    "        acc, rmse=test_model_get_results(encoder,mlp_model, validation_x, validation_y, False, sub_tag, file)\n",
    "        acc_list.append(acc)\n",
    "        rmse_list.append(rmse)\n",
    "        print(\"testing on :\", test_sub_list[sub_iter])\n",
    "        file.write(\"Testing  Result :\")\n",
    "        file.write(test_sub_list[sub_iter])\n",
    "        file.write(\"\\n\")\n",
    "        # all_data=get_data_frames_from_files(path, file_names, subject_dict, [test_sub_list[sub_iter]])\n",
    "        all_data = df_dict[test_sub_list[sub_iter]]\n",
    "        test_x, test_y, validation_x, validation_y = get_train_data_from_df(all_data, 0.25)\n",
    "        acc, rmse=test_model_get_results(encoder,mlp_model, test_x, test_y, True, sub_tag, file)\n",
    "        # tf.keras.backend.clear_session()\n",
    "\n",
    "        test_acc_list.append(acc)\n",
    "        test_rmse_list.append(rmse)\n",
    "        # break\n",
    "        # file.write(acc)\n",
    "        file.write(\"\\n\")\n",
    "        cnt=cnt+1\n",
    "        sub_file=open(sub_cnt_file,'wb')\n",
    "        pickle.dump(cnt, sub_file)\n",
    "        sub_file.close()\n",
    "        # if sub_iter>=5:\n",
    "        # break\n",
    "    file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_gait_phase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
